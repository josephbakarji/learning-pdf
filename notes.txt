
TO DO:

	- write code that creates examples with various initial conditions 
	- write filter code in DataIO that picks out files with certain properties
        - examples: linear u0, gaussian k, same discretization (dU, dx, dt)

	- test impact of range on variable coefficient

    - Streamline code
        - separate simlator from learner in testfiles
        - Add initial condition training examples 
            - train on examples with the same discretization (dU, dx, dt)
        - fix vizualization module
        - replace casefolder by casedir = DATAFILE + casefolder, and when relevant casefolder with case

    - print features: write function makeFeatureNames (includes ^{01} -> 1*x)
    - Improve derivatives: Implement total variation regularized derivative

    - Test it on simple reaction case: du/dt = ku

    - Evaluate errors
        - analyze distribution of errors AND distribution of derivatives: we want the derivatives to be equaly represented
        - error should be dependent on increments dU, dx, and dt. Quantify order of error

    - Include integral term in features
    
    - Test if ranges of U, x, t makes a difference for variable coefficient learning

    - Test on 5-equation model


Some more improvements:
    - Code:
        - Optimize makefeatures function options (too redundant)

    - Method:
        - Compute derivatives analytically for better estimate of accuaracy
        - Learn nonlocal terms (integrals)

	- Visualization: 
		- Unify 2D plots (plot_flabel and plot_fu, x and t)
		- make global self.slider list and append sliders to it to avoid conflicts


-----------------------------------------------------------
Issues And Discussion:

    - Storage:
        - Store in JSON format from dictionary in separate file
        - Metadata to include:
            - filename: keep the same?
            - discretization size (or number of grid points)
            - number of dimensions
            - ranges 
            - fu0_distribution: gaussian, uniform
                - parameters: (mean, variance) for gaussian, (xmin, xmax) for uniform etc.
            - fk_distribution: gaussian, uniform
                - parameters: (mean, variance) for gaussian, (xmin, xmax) for uniform etc.
            - u0: line, exp, sine etc.
                - parameters: (a, b) for ax+b if line, (a, b) for a*exp(bx) exp, etc. 

    - Robustness Analysis
        - The robustness (and thus generalizability) of the method depends on the distribution of the derivatives being all equally represented. 
        - If a stencil set of elements is the same, it's technically a redundant training example.

    - Code Issues:
        - Error doesn't change dramatically for a zero closure term.
        - Closure term fluctuates around zero when plotted
        - Error increases with T (training set size) for u0 exponential - Could be due to high nonlinearity at larger times.
        - What's the difference between score and error? 

    - Method:
        - The solution domain should contain the whole PDF to learn better: this could be computationally expensive 
        - A small error in the coefficient will cause a large one when integrated.

-----------------------------------------------------------

Paper:
    - Results:
        - Generalize in time
            - Test on gradually farther portions of time.
            - Now time_general measures error as a function of training set size
        - Coefficient as a funciton of lambda
        - Sensitivity analysis
        - Error as a function of number of terms
        - Generalization tests: error as a function of training/test generality
        - Predictive ability vs. parsimony (as done in lipson's paper)

-------------------------------------------------------------


New Ideas:
    - Learn the dynamics (equation) of a kinetic defect f(x, t) in the case of shocks.
    - Learn integrator using neural network.
