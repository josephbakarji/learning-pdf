
TO DO:


    - function for picking out conditions
    - function for learning from that set 
    - examples: linear u0, gaussian k, same discretization (dU, dx, dt)

    ---
    
    - Get Results:
        - Test impact of range on variable coefficient
        - Evaluate errors
            - analyze distribution of errors AND distribution of derivatives: we want the derivatives to be equaly represented
            - error should be dependent on increments dU, dx, and dt. Quantify order of error
        - Plot histogram comparing results of various combinations of properties

    - Algorithm Improvements:
        - Improve derivatives: Implement total variation regularized derivative
        - Include integral term in features

    - Vizualization:
        - print features: write function makeFeatureNames (includes ^{01} -> 1*x)
        - fix vizualization module

    - Test cases:
        - Try simple reaction case: du/dt = ku
        - Test on 5-equation model


Some more improvements:
    - Code:
        - Optimize makefeatures function options (too redundant)

    - Method:
        - Compute derivatives analytically for better estimate of accuaracy
        - Learn nonlocal terms (integrals)

	- Visualization: 
		- Unify 2D plots (plot_flabel and plot_fu, x and t)
		- make global self.slider list and append sliders to it to avoid conflicts


-----------------------------------------------------------
Issues And Discussion:

    - Storage:
        - Store in JSON format from dictionary in separate file
        - Metadata to include:
            - filename: keep the same?
            - discretization size (or number of grid points)
            - number of dimensions
            - ranges 
            - fu0_distribution: gaussian, uniform
                - parameters: (mean, variance) for gaussian, (xmin, xmax) for uniform etc.
            - fk_distribution: gaussian, uniform
                - parameters: (mean, variance) for gaussian, (xmin, xmax) for uniform etc.
            - u0: line, exp, sine etc.
                - parameters: (a, b) for ax+b if line, (a, b) for a*exp(bx) exp, etc. 

    - Robustness Analysis
        - The robustness (and thus generalizability) of the method depends on the distribution of the derivatives being all equally represented. 
        - If a stencil set of elements is the same, it's technically a redundant training example.

    - Code Issues:
        - Error doesn't change dramatically for a zero closure term.
        - Closure term fluctuates around zero when plotted
        - Error increases with T (training set size) for u0 exponential - Could be due to high nonlinearity at larger times.
        - What's the difference between score and error? 

    - Method:
        - The solution domain should contain the whole PDF to learn better: this could be computationally expensive 
        - A small error in the coefficient will cause a large one when integrated.

-----------------------------------------------------------

Paper:
    - Results:
        - Generalize in time
            - Test on gradually farther portions of time.
            - Now time_general measures error as a function of training set size
        - Coefficient as a funciton of lambda
        - Sensitivity analysis
        - Error as a function of number of terms
        - Generalization tests: error as a function of training/test generality
        - Predictive ability vs. parsimony (as done in lipson's paper)

-------------------------------------------------------------


New Ideas:
    - Learn the dynamics (equation) of a kinetic defect f(x, t) in the case of shocks.
    - Learn integrator using neural network.
